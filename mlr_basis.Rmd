---
title: "Multilinear Regression: basis"
author: "Rob van der Nagel"
date: "12-3-19"
output:
  html_notebook:
    df_print: paged
    highlight: pygments
    toc: yes
    toc_float:
      collapsed: false
  html_document:
    highlight: pygments
    toc: yes
  pdf_document:
    highlight: pygments
    toc: yes
urlcolor: blue
---

```{r, include = F}
rm(list = ls())
source('lib/utils.r')
```
```{r, child = c('lib/macros.tex')}
```

# Model linear regression

See also [M&R, sections 12-1 to 12-4] ().
Given $ \ beta: = (\ beta_j) _ {j = 0} ^ k $ and $ \ sigma> 0 $.
We model a population of probability variables $ (Y, X_1, \ dotsc, X_k, \ varepsilon) $ with the following properties:
$$
  Y = \ beta_0 + \ beta_1 X_1 + \ dotso + \ beta_k X_k + E
  \ ,,
$$
with $ E \ DistAs \ Normal {0, \ sigma} $.
Written the other way around, it is
$$
  (Y \ mid X = x) \ DistAs \ Normal {\ beta_0 + \ beta_1 x_i + \ dotso + \ beta_k x_k, \ sigma}
  \ ,.
$$

This means that at the level of a sample from this population
$$
  y_i = \ beta_0 + \ beta_1 x_ {1, i} + \ dotso + \ beta_k x_ {k, i} + \ varepsilon_i
  \ qquad \ forall {} i \ in \ Set {1, \ dotsc, n}
  \ ,.
$$

The problem is to estimate the parameters $ \ beta $ and $ \ sigma $.
The estimation is done using the 'least squares method' and is in [M&R, sections 12-1.2 and 12-1.3, (12-13)] ().

Analysis of the model:

* A $ F $ key for $ H_0: \ beta_1 = \ beta_2 = \ dotso = \ beta_k = 0 $ in [M&R, section 12-2.1 and (12-18)] ().
* The $ t $ keys for $ H_0: \ beta_i = 0 $ in [M&R, section 12.-2.2 and (12-24)] ().
* Confidence intervals for $ \ beta_i $ in [M&R, section 12-3] ().
* An analysis of the residues (= _ remains) on normal distribution via quantile-quantile plot [M&R, section 12-5] () and Shapiro-Wilk test.
* The _proportion explained variance_ $ R ^ 2 $ [M&R, (12-23)] () with a correction for the number of parameters representing the proportion of the variance predicted by the model.
* Outlier analysis with _Cook's distance_ [M&R, (12-45)] ().
* Make predictions [M&R, section 12-4] ().

The $ F $ key and $ R ^ 2 $ follow from the independence of the error:
$$
  \ Variance (Y) = \ Variance (\ beta_1 X_1 + \ dotso + \ beta_k X_k) + \ Variance (E)
  \ ,.
$$

Three reasons for multi-linear models:
1. Easy to understand because only linear algebra.
2. Performant on computers to calculate: $ k = 100 $ and $ n = 10000 $ no problem on a laptop.
3. More complex regression models can be linearly approximated individually.

# Example optimum situation
```{r}
o.n <- 250
o.x1 <- rt(o.n, df = 5)
o.x2 <- runif(o.n, -2, 3)
o.x3 <- runif(o.n, -1, 3) ^ 2
o.x4 <- rexp(o.n, 1/3)
o.sigma <- 2
o.e <- rnorm(o.n, 0, sd = o.sigma)
o.beta.0 <- 5
o.beta.1 <- -3
o.beta.2 <- -1
o.beta.3 <- 0.01
o.beta.4 <- 2
o.beta <- c(o.beta.0, o.beta.1, o.beta.2, o.beta.3, o.beta.4)
o.k <- length(o.beta) - 1
o.y <- o.beta.0 + o.beta.1 * o.x1 + o.beta.2 * o.x2 + o.beta.3 * o.x3 + o.beta.4 * o.x4 + o.e
o.df <- data.frame(y = o.y, x1 = o.x1, x2 = o.x2, x3 = o.x3, x4 = o.x4)
```

## Exploring data

`` `{r}
str (o.df)
summary (o.df)
`` `
`` `{r}
library ('corrplot')
corrplot :: corrplot (cor (o.df))
`` `


`` `{r}
pairs (o.df)
`` `

## Model with R

`` `{r}
o.lm <- 1m (y ~ x1 + x2 + x3 + x4, data = o.df)
`` `

`` `{r}
o.beta
coefficients (o.lm)
`` `

`` `{r}
summary (o.lm)
`` `

## Origin of statistics

Sum of squares [M&R, (12-17), (12-20) and (12-21)] ():
`` `{r}
o.sk.e <- sum (o.lm $ residuals ^ 2)
o.sk.t <- sum (o.y ^ 2) - o.n * ​​mean (o.y) ^ 2
`` `

The estimator for the $ \ sigma ^ 2 $ [M&R, (12-16)] () has been corrected with $ \ frac {1} {`r on - ok - 1`} $ instead of $ \ frac {1} {`r on - 1`} $.
For a large number of terms this makes a difference.
`` `{r}
sigma
summary (o.lm) $ sigma
sqrt (o.sk.e / (o.n - o.k - 1))
sd (o.lm $ residuals)
sqrt (sd (o.lm $ residuals) ^ 2 * (o.n - 1) / (o.n - length (o.beta)))
`` `

The $ R ^ 2 $ statistic [M&R, (12-22)] () has been adjusted to $ R ^ 2_ {adj} $ [M&R, (12-23)] ().
The reason is that more terms make it easier to fit a model.
So they want to pay a price for more terms by a lower $ R ^ 2_ {adj} $.
`` `{r}
o.r <- 1 - o.sk.e / o.sk.t
o.r
o.r.adj <- 1 - o.sk.e * (o.n - 1) / o.sk.t / (o.n - o.k - 1)
o.r.adj
`` `
The $ F $ statistic comes from an ANOVA with
$$ H_0: \ beta_1 = \ dotso = \ beta_k = 0 \.,
So this tests whether there is more than one constant model.

`` `{r}
o.f <- ((o.sk.t - o.sk.e) / o.k) / (o.sk.e / (o.n - o.k - 1))
or
`` `
## Analysis of residues

`` `{r}
plot (o.lm)
`` `

## Prediction and reliability

`` `{r}
o.n2 <- 50
o.df2 <- data.frame (x1 = runif (o.n2, -1, 20)
                   , x2 = runif (o.n2, -3, 4)
                   , x3 = runif (o.n2, -1, 10)
                   , x4 = runif (o.n2, -2, 20))
o.bi.c <- predict (o.lm, o.df2, interval = "confidence")
plot (o.bi.c [, "fit"], o.bi.c [, "fit"], pch = 'x', col = 'red', ylab = 'real', xlab = 'fit', main = 'Confidence intervals and simulation')
lines (o.bi.c [, "fit"], o.bi.c [, "lwr"], col = 'blue')
lines (o.bi.c [, "fit"], o.bi.c [, "upr"], col = 'blue')
o.bi.p <- predict (o.lm, o.df2, interval = "prediction")
lines (o.bi.c [, "fit"], o.bi.p [, "lwr"], col = "green")
lines (o.bi.c [, "fit"], o.bi.p [, "upr"], col = "green")
points (o.bi.c [, "fit"], o.bi.c [, "fit"] + rnorm (o.n2, 0, sd = summary (o.lm) $ sigma), pch = 'o ")
`` `

# Example reel data

See [M&R, exercise 12-5] ().
`` `{r}
p.df <- load.mr.mnemonic ('M&R 12E-1 patient satisfaction')
`` `
## Exploring data

`` `{r}
str (p.df)
summary (p.df)
`` `

`` `{r}
library ('corrplot')
corrplot :: corrplot (cor (p.df))
pairs (p.df)
`` `

## Model with R

`` `{r}
p.lm <- lm (satisfaction ~ age + severity + anxiety, data = p.df)
`` `
`` `{r}
summary (p.lm)
`` `

`` `{r}
plot (p.lm)
`` `

`` `{r}
library ('car')
car :: outlierTest (p.lm)
`` `
